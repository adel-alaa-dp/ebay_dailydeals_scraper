# ğŸ“¦ eBay Daily Deals Scraper (Scrapy Project)

A professional-grade Scrapy spider that scrapes the latest daily deals from [eBay Global Deals](https://www.ebay.com/globaldeals), exporting data to:

- ğŸ“‚ Local CSV file (`exports/` folder)
- ğŸ“„ Google Sheets (optional)

---

## ğŸš€ Features

- Scrapes all daily deal categories
- Handles AJAX paginated content
- Supports retries with exponential backoff
- Exports to both CSV and Google Sheets
- Logs all activities to `logs/`
- Duplicate product detection (SKU-based)
- Professional project structure

---

## ğŸ›  Installation

```bash
# Clone the repository
$ git clone https://github.com/your_username/ebay_dailydeals_scraper.git
$ cd ebay_dailydeals_scraper

# Install dependencies
$ pip install -r requirements.txt
```

Dependencies:
- `scrapy`
- `gspread`
- `oauth2client`
- `lxml`

```bash
pip install scrapy gspread oauth2client lxml
```

---

## âš™ï¸ Project Structure

```
.
â”œâ”€â”€ ebay_dailydeals/
â”‚   â”œâ”€â”€ items.py
â”‚   â”œâ”€â”€ pipelines.py
â”‚   â”œâ”€â”€ settings.py
â”‚   â””â”€â”€ spiders/
â”‚       â””â”€â”€ daily_deals_spider.py
â”œâ”€â”€ exports/
â”œâ”€â”€ logs/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ run_spider.py
â”œâ”€â”€ scrapy.cfg  
â””â”€â”€ README.md

```

---

## ğŸ“‹ Usage

```bash
python run_spider.py
```

After crawling:
- CSV is saved in `exports/`
- Google Sheet is updated if credentials are configured

> **Note:** To enable Google Sheets export, place your service account JSON file in the root directory and update its filename in `pipelines.py`.
> **Note:** Place our proxy in `middlewares.py`.

---

## ğŸ§¹ To-Do
- [ ] Add Dockerfile for containerization
- [ ] Add GitHub Actions CI/CD workflow
- [ ] Extend spider to handle different regions (UK, DE, etc.)

---

## ğŸ§‘â€ğŸ’» Author

- Built by [Adel Alaa](https://github.com/adel-alaa-dp)
- Linkedin: https://www.linkedin.com/in/adel-alaa/
